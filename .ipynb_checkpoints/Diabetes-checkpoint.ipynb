{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>marital</th>\n",
       "      <th>income</th>\n",
       "      <th>household_size</th>\n",
       "      <th>...</th>\n",
       "      <th>trigs</th>\n",
       "      <th>wbc</th>\n",
       "      <th>hgb</th>\n",
       "      <th>hct</th>\n",
       "      <th>platelets</th>\n",
       "      <th>s_cotinine</th>\n",
       "      <th>a1c</th>\n",
       "      <th>hdl</th>\n",
       "      <th>grip_strength</th>\n",
       "      <th>fev1_fvc_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69220</td>\n",
       "      <td>Gwendolyn</td>\n",
       "      <td>Runolfsson</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.654</td>\n",
       "      <td>5.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63030</td>\n",
       "      <td>Augustus</td>\n",
       "      <td>Farrell</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>15.1</td>\n",
       "      <td>44.4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.221</td>\n",
       "      <td>5.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64051</td>\n",
       "      <td>Aaron</td>\n",
       "      <td>Schmeler</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>41.3</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65141</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Bechtelar</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>14.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>16.300</td>\n",
       "      <td>5.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64632</td>\n",
       "      <td>Hayden</td>\n",
       "      <td>Brekke</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>15.6</td>\n",
       "      <td>45.1</td>\n",
       "      <td>306.0</td>\n",
       "      <td>212.000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn      first        last  gender  age  race  education  marital  \\\n",
       "0  69220  Gwendolyn  Runolfsson       1   21     7        2.0      6.0   \n",
       "1  63030   Augustus     Farrell       0   21     1        2.0      6.0   \n",
       "2  64051      Aaron    Schmeler       0   21     2        3.0      5.0   \n",
       "3  65141        Bob   Bechtelar       0   21     1        2.0      5.0   \n",
       "4  64632     Hayden      Brekke       0   21     2        3.0      5.0   \n",
       "\n",
       "   income  household_size  ...  trigs  wbc   hgb   hct  platelets  s_cotinine  \\\n",
       "0     1.0               2  ...   54.0  6.0  12.7  36.1      157.0       0.654   \n",
       "1     3.0               4  ...   83.0  6.9  15.1  44.4      226.0       0.221   \n",
       "2     4.0               3  ...  256.0  8.2  14.4  41.3      266.0       0.011   \n",
       "3     4.0               4  ...   57.0  6.6  14.7  43.0      206.0      16.300   \n",
       "4    10.0               2  ...   70.0  7.8  15.6  45.1      306.0     212.000   \n",
       "\n",
       "   a1c   hdl  grip_strength  fev1_fvc_ratio  \n",
       "0  5.0  47.0           50.3            0.78  \n",
       "1  5.2  40.0           90.1            0.84  \n",
       "2  5.1  38.0           72.7            0.83  \n",
       "3  5.1  55.0           86.6            0.83  \n",
       "4  6.0  39.0           94.4            0.83  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "df=pd.read_csv(\"Resources/Adults_Diabetes_NHANES_2011_2012.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seqn</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>marital</th>\n",
       "      <th>income</th>\n",
       "      <th>household_size</th>\n",
       "      <th>insurance</th>\n",
       "      <th>gen_health</th>\n",
       "      <th>...</th>\n",
       "      <th>glob</th>\n",
       "      <th>trigs</th>\n",
       "      <th>wbc</th>\n",
       "      <th>hgb</th>\n",
       "      <th>hct</th>\n",
       "      <th>platelets</th>\n",
       "      <th>s_cotinine</th>\n",
       "      <th>a1c</th>\n",
       "      <th>hdl</th>\n",
       "      <th>grip_strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69220</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.654</td>\n",
       "      <td>5.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63030</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>83.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>15.1</td>\n",
       "      <td>44.4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.221</td>\n",
       "      <td>5.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64051</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>41.3</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>72.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65141</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>57.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>14.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>16.300</td>\n",
       "      <td>5.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>86.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64632</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>15.6</td>\n",
       "      <td>45.1</td>\n",
       "      <td>306.0</td>\n",
       "      <td>212.000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>94.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    seqn  gender  age  race  education  marital  income  household_size  \\\n",
       "0  69220       1   21     7        2.0      6.0     1.0               2   \n",
       "1  63030       0   21     1        2.0      6.0     3.0               4   \n",
       "2  64051       0   21     2        3.0      5.0     4.0               3   \n",
       "3  65141       0   21     1        2.0      5.0     4.0               4   \n",
       "4  64632       0   21     2        3.0      5.0    10.0               2   \n",
       "\n",
       "   insurance  gen_health  ...  glob  trigs  wbc   hgb   hct  platelets  \\\n",
       "0          1         3.0  ...   2.9   54.0  6.0  12.7  36.1      157.0   \n",
       "1          2         3.0  ...   2.8   83.0  6.9  15.1  44.4      226.0   \n",
       "2          1         3.0  ...   3.0  256.0  8.2  14.4  41.3      266.0   \n",
       "3          1         4.0  ...   2.6   57.0  6.6  14.7  43.0      206.0   \n",
       "4          1         2.0  ...   2.9   70.0  7.8  15.6  45.1      306.0   \n",
       "\n",
       "   s_cotinine  a1c   hdl  grip_strength  \n",
       "0       0.654  5.0  47.0           50.3  \n",
       "1       0.221  5.2  40.0           90.1  \n",
       "2       0.011  5.1  38.0           72.7  \n",
       "3      16.300  5.1  55.0           86.6  \n",
       "4     212.000  6.0  39.0           94.4  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1=df.drop(columns=['first', 'last','drinks_day', 'depression','fev1_fvc_ratio'])\n",
    "#Remember to Add back SEQN for ETL\n",
    "\n",
    "df_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5206"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df_1.dropna()\n",
    "len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmi</th>\n",
       "      <th>bmi_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>32.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>30.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>26.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>37.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>33.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>24.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>30.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>29.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>27.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4720</th>\n",
       "      <td>30.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>54.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>31.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3879</th>\n",
       "      <td>27.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>33.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>28.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>23.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4548</th>\n",
       "      <td>29.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>34.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>20.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>18.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>24.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4359</th>\n",
       "      <td>23.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>24.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>34.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>39.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>33.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>25.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>38.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>33.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>21.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>30.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>35.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>33.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>23.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>24.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>37.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>24.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4523</th>\n",
       "      <td>31.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>31.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>37.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>25.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>32.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>26.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4718</th>\n",
       "      <td>30.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>28.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bmi  bmi_group\n",
       "1599  32.1          2\n",
       "2552  30.6          2\n",
       "424   26.3          1\n",
       "2356  37.1          2\n",
       "2810  33.6          2\n",
       "1817  24.4          0\n",
       "2596  30.6          2\n",
       "2750  29.3          1\n",
       "1350  27.9          1\n",
       "4720  30.2          2\n",
       "3156  54.5          2\n",
       "2075  31.3          2\n",
       "3879  27.6          1\n",
       "1716  33.3          2\n",
       "2600  28.1          1\n",
       "1719  23.3          0\n",
       "4548  29.9          1\n",
       "1107  18.6          0\n",
       "3610  34.1          2\n",
       "2729  20.5          0\n",
       "360   18.1          0\n",
       "148   24.6          0\n",
       "4359  23.2          0\n",
       "578   24.2          0\n",
       "3313  34.4          2\n",
       "846   39.3          2\n",
       "2065  33.3          2\n",
       "808   25.2          1\n",
       "275   38.5          2\n",
       "1162  33.8          2\n",
       "562   21.7          0\n",
       "802   22.0          0\n",
       "3316  30.3          2\n",
       "2340  35.5          2\n",
       "4081  33.6          2\n",
       "1363  23.9          0\n",
       "1128  24.0          0\n",
       "1701  24.9          0\n",
       "4156  37.1          2\n",
       "140   24.9          0\n",
       "4523  31.6          2\n",
       "2839  31.4          2\n",
       "4223  37.0          2\n",
       "4633  25.3          1\n",
       "453   18.0          0\n",
       "1614  32.5          2\n",
       "16    22.0          0\n",
       "1570  26.2          1\n",
       "4718  30.3          2\n",
       "4546  28.3          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BMI \n",
    "#0= BMI less than 25\n",
    "#1= BMI less than 30 and BMI great than 24\n",
    "#2=BMI great than 30\n",
    "\n",
    "conditions = [\n",
    "   df1['bmi'] <25,\n",
    "    (df1['bmi'] >=25) & (df1['bmi'] <30),\n",
    "   df1['bmi'] >=30\n",
    "]\n",
    "choices = [0,1, 2]\n",
    "df1['bmi_group'] = np.select(conditions, choices, default=1)\n",
    "\n",
    "#df1['bmi_group'] = 1\n",
    "#df1.loc[df1['bmi'] >=30,'bmi_group'] = 2\n",
    "#df1.loc[df1['bmi'] <24,'bmi_group']= 0\n",
    "\n",
    "df1[[\"bmi\",\"bmi_group\"]].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4656</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4624</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4946</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4850</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3065</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4944</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>54</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4478</th>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5110</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3921</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  age_group\n",
       "3068   54          3\n",
       "4656   74          4\n",
       "1061   32          2\n",
       "882    30          1\n",
       "4624   74          4\n",
       "3061   54          3\n",
       "1827   40          2\n",
       "3541   60          3\n",
       "3531   60          3\n",
       "525    26          1\n",
       "3747   62          3\n",
       "687    28          1\n",
       "3891   63          3\n",
       "5012   80          4\n",
       "4946   80          4\n",
       "54     21          1\n",
       "4850   80          4\n",
       "2794   51          3\n",
       "446    25          1\n",
       "899    30          1\n",
       "2421   47          3\n",
       "1318   35          2\n",
       "3065   54          3\n",
       "1164   33          2\n",
       "1142   33          2\n",
       "4944   80          4\n",
       "768    29          1\n",
       "3069   54          3\n",
       "223    23          1\n",
       "4420   70          4\n",
       "423    25          1\n",
       "4901   80          4\n",
       "1984   42          2\n",
       "2089   43          2\n",
       "3975   64          3\n",
       "2307   46          3\n",
       "630    27          1\n",
       "3455   59          3\n",
       "542    26          1\n",
       "608    27          1\n",
       "1439   36          2\n",
       "355    24          1\n",
       "1997   42          2\n",
       "2054   43          2\n",
       "4478   71          4\n",
       "626    27          1\n",
       "5110   80          4\n",
       "3880   63          3\n",
       "2275   45          2\n",
       "3921   63          3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Age Group \n",
    "\n",
    "# 0=  0-20\n",
    "# 1=  21-30\n",
    "# 2=  31-45\n",
    "# 3=  46-65\n",
    "# 4=  65+\n",
    "conditions1 = [\n",
    "    (df1['age'] >20)&(df1['age'] <31),\n",
    "    (df1['age'] >30)&(df1['age'] <46),\n",
    "    (df1['age'] >45)&(df1['age'] <66),\n",
    "    df1['age'] >65\n",
    "]\n",
    "choices1 = [1, 2, 3,4]\n",
    "df1['age_group'] = np.select(conditions1, choices1, default=0)\n",
    "\n",
    "\n",
    "#df1['age_group'] = 0\n",
    "#df1.loc[(df1['age'] >20)&(df1['age'] <31),'age_group'] = 1\n",
    "#df1.loc[(df1['age'] >30)&(df1['age'] <46),'age_group']= 2\n",
    "#df1.loc[(df1['age'] >45)&(df1['age'] <66),'age_group'] = 3\n",
    "#df1.loc[df1['age'] >65,'age_group'] = 4\n",
    "\n",
    "df1[[\"age\",\"age_group\"]].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sys_bp</th>\n",
       "      <th>dia_bp</th>\n",
       "      <th>bp_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>120.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>116.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>114.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>120.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4378</th>\n",
       "      <td>116.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>190.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>144.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>154.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>134.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>156.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4404</th>\n",
       "      <td>116.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4511</th>\n",
       "      <td>126.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4632</th>\n",
       "      <td>122.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4587</th>\n",
       "      <td>154.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>124.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3386</th>\n",
       "      <td>128.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>114.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>106.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>124.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>120.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3339</th>\n",
       "      <td>148.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>166.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>116.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3367</th>\n",
       "      <td>108.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>118.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4623</th>\n",
       "      <td>126.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>112.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>102.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>128.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>118.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>102.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>114.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>114.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3667</th>\n",
       "      <td>150.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4057</th>\n",
       "      <td>132.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4465</th>\n",
       "      <td>100.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>110.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3410</th>\n",
       "      <td>106.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>120.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3395</th>\n",
       "      <td>110.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>164.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>120.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>116.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>122.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>158.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>112.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>134.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>108.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3524</th>\n",
       "      <td>144.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>114.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sys_bp  dia_bp  bp_group\n",
       "292    120.0    70.0         1\n",
       "62     116.0    80.0         2\n",
       "4386   114.0    50.0         0\n",
       "1147   120.0    84.0         2\n",
       "4378   116.0    52.0         0\n",
       "4912   190.0    70.0         2\n",
       "3035   144.0    66.0         2\n",
       "4329   154.0    78.0         2\n",
       "116    134.0    46.0         2\n",
       "4851   156.0    76.0         2\n",
       "4404   116.0    64.0         0\n",
       "4511   126.0    64.0         1\n",
       "4632   122.0    72.0         1\n",
       "4587   154.0    90.0         2\n",
       "4161   124.0    84.0         2\n",
       "3386   128.0    88.0         2\n",
       "158    114.0    76.0         0\n",
       "119    106.0    66.0         0\n",
       "317    124.0    74.0         1\n",
       "4447   120.0    74.0         1\n",
       "3339   148.0    74.0         2\n",
       "2920   166.0   108.0         2\n",
       "1791   116.0    80.0         2\n",
       "3367   108.0    58.0         0\n",
       "547    118.0    66.0         0\n",
       "4623   126.0    64.0         1\n",
       "1061   112.0    74.0         0\n",
       "1556   102.0    62.0         0\n",
       "3899   128.0    76.0         1\n",
       "1793   118.0    70.0         0\n",
       "535    102.0    56.0         0\n",
       "4638   114.0    62.0         0\n",
       "809    114.0    68.0         0\n",
       "3667   150.0    82.0         2\n",
       "4057   132.0    76.0         2\n",
       "4465   100.0    58.0         0\n",
       "471    110.0    54.0         0\n",
       "3410   106.0    64.0         0\n",
       "347    120.0    58.0         1\n",
       "3395   110.0    72.0         0\n",
       "1629   164.0    84.0         2\n",
       "751    120.0    74.0         1\n",
       "817    116.0    74.0         0\n",
       "4993   122.0    72.0         1\n",
       "1389   158.0   100.0         2\n",
       "3132   112.0    62.0         0\n",
       "4439   134.0    68.0         2\n",
       "792    108.0    52.0         0\n",
       "3524   144.0    86.0         2\n",
       "2563   114.0    86.0         2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Blood Pressure Group \n",
    "#Systolic Blood Pressure less than 120 and Diastolic Blood Pressure of less than 80\n",
    "#Systolic Blood Pressure between 120 and 129 and Diastolic Blood Pressure of less than 80\n",
    "#Systolic Blood Pressure greater than 130 or  Diastolic Blood Pressure of greater than 80\n",
    "\n",
    "conditions2 = [\n",
    "    (df1['sys_bp'] <120)&(df1['dia_bp'] <80),\n",
    "   (df1['sys_bp'] >119)&(df1['sys_bp'] <=129)&(df1['dia_bp'] <80),\n",
    "    (df1['sys_bp'] >129)|(df1['dia_bp'] >=80),\n",
    " \n",
    "]\n",
    "choices2 = [0, 1, 2]\n",
    "df1['bp_group'] = np.select(conditions2, choices2, default=0)\n",
    "#df1['bp_group'] = 0\n",
    "#df1.loc[(df['sys_bp'] <120)&(df1['dia_bp'] <80),'bp_group'] = 0\n",
    "#df1.loc[(df['sys_bp'] >119)&(df1['sys_bp'] <=129)&(df['dia_bp'] <80),'bp_group'] = 1\n",
    "#df1.loc[(df['sys_bp'] >129)|(df1['dia_bp'] >=80),'bp_group'] = 2\n",
    "\n",
    "\n",
    "df1[[\"sys_bp\",\"dia_bp\",\"bp_group\"]].sample(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1.drop(columns=['seqn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"diabetes\"]= df2[\"diabetes\"]-1\n",
    "\n",
    "target = df2[\"diabetes\"]\n",
    "target_names = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>education</th>\n",
       "      <th>marital</th>\n",
       "      <th>income</th>\n",
       "      <th>household_size</th>\n",
       "      <th>insurance</th>\n",
       "      <th>gen_health</th>\n",
       "      <th>asthma</th>\n",
       "      <th>...</th>\n",
       "      <th>hgb</th>\n",
       "      <th>hct</th>\n",
       "      <th>platelets</th>\n",
       "      <th>s_cotinine</th>\n",
       "      <th>a1c</th>\n",
       "      <th>hdl</th>\n",
       "      <th>grip_strength</th>\n",
       "      <th>bmi_group</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bp_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.654</td>\n",
       "      <td>5.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.1</td>\n",
       "      <td>44.4</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.221</td>\n",
       "      <td>5.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.4</td>\n",
       "      <td>41.3</td>\n",
       "      <td>266.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5.1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>16.300</td>\n",
       "      <td>5.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.6</td>\n",
       "      <td>45.1</td>\n",
       "      <td>306.0</td>\n",
       "      <td>212.000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age  race  education  marital  income  household_size  insurance  \\\n",
       "0       1   21     7        2.0      6.0     1.0               2          1   \n",
       "1       0   21     1        2.0      6.0     3.0               4          2   \n",
       "2       0   21     2        3.0      5.0     4.0               3          1   \n",
       "3       0   21     1        2.0      5.0     4.0               4          1   \n",
       "4       0   21     2        3.0      5.0    10.0               2          1   \n",
       "\n",
       "   gen_health  asthma  ...   hgb   hct  platelets  s_cotinine  a1c   hdl  \\\n",
       "0         3.0     2.0  ...  12.7  36.1      157.0       0.654  5.0  47.0   \n",
       "1         3.0     1.0  ...  15.1  44.4      226.0       0.221  5.2  40.0   \n",
       "2         3.0     2.0  ...  14.4  41.3      266.0       0.011  5.1  38.0   \n",
       "3         4.0     2.0  ...  14.7  43.0      206.0      16.300  5.1  55.0   \n",
       "4         2.0     2.0  ...  15.6  45.1      306.0     212.000  6.0  39.0   \n",
       "\n",
       "   grip_strength  bmi_group  age_group  bp_group  \n",
       "0           50.3          0          1         0  \n",
       "1           90.1          1          1         0  \n",
       "2           72.7          1          1         0  \n",
       "3           86.6          0          1         0  \n",
       "4           94.4          2          1         2  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df2.drop(\"diabetes\", axis=1) \n",
    "feature_names = data.columns \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8503067484662576"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier() \n",
    "clf = clf.fit(X_train, y_train) \n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226993865030675"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf = RandomForestClassifier(n_estimators=75) \n",
    "rf = rf.fit(X_train, y_train) \n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp=sorted(zip(rf.feature_importances_, feature_names), reverse=True)\n",
    "\n",
    "feature_imp[1][0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1c',\n",
       " 'glucose',\n",
       " 'alb_cr_ratio',\n",
       " 't_chol',\n",
       " 'trigs',\n",
       " 'platelets',\n",
       " 'grip_strength',\n",
       " 'ldh',\n",
       " 'cr',\n",
       " 'alk_phos',\n",
       " 'wbc',\n",
       " 'potassium',\n",
       " 'alt',\n",
       " 'hct',\n",
       " 'bun',\n",
       " 'ca',\n",
       " 'u_acid',\n",
       " 'cpk',\n",
       " 'sodium',\n",
       " 'hdl',\n",
       " 'hgb',\n",
       " 'glob',\n",
       " 'ast',\n",
       " 't_protein',\n",
       " 'iron',\n",
       " 'hypertension',\n",
       " 'gen_health',\n",
       " 'phos',\n",
       " 's_cotinine',\n",
       " 'chloride',\n",
       " 't_bilirubin']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remlist=['age', 'bmi','height_cm','waist_cm','weight_kg','dia_bp','sys_bp']\n",
    "feats= []\n",
    "\n",
    "for x in feature_imp: \n",
    "    if x[0]>0.01 and x[1] not in remlist:\n",
    "        feats.append(x[1])\n",
    "        \n",
    "\n",
    "\n",
    "feats        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a1c',\n",
       " 'glucose',\n",
       " 'alb_cr_ratio',\n",
       " 't_chol',\n",
       " 'trigs',\n",
       " 'platelets',\n",
       " 'grip_strength',\n",
       " 'ldh',\n",
       " 'cr',\n",
       " 'alk_phos',\n",
       " 'wbc',\n",
       " 'potassium',\n",
       " 'alt',\n",
       " 'hct',\n",
       " 'bun',\n",
       " 'ca',\n",
       " 'u_acid',\n",
       " 'cpk',\n",
       " 'sodium',\n",
       " 'hdl',\n",
       " 'hgb',\n",
       " 'glob',\n",
       " 'ast',\n",
       " 't_protein',\n",
       " 'iron',\n",
       " 'hypertension',\n",
       " 'gen_health',\n",
       " 'phos',\n",
       " 's_cotinine',\n",
       " 'chloride',\n",
       " 't_bilirubin',\n",
       " 'bmi_group',\n",
       " 'age_group',\n",
       " 'bp_group',\n",
       " 'diabetes']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_list=[\"bmi_group\", \"age_group\", \"bp_group\",\"diabetes\"]\n",
    "\n",
    "for x in group_list: \n",
    "    if x not in feats: \n",
    "        feats.append(x)\n",
    "        \n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_df =df2[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1c</th>\n",
       "      <th>glucose</th>\n",
       "      <th>alb_cr_ratio</th>\n",
       "      <th>t_chol</th>\n",
       "      <th>trigs</th>\n",
       "      <th>platelets</th>\n",
       "      <th>grip_strength</th>\n",
       "      <th>ldh</th>\n",
       "      <th>cr</th>\n",
       "      <th>alk_phos</th>\n",
       "      <th>...</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>gen_health</th>\n",
       "      <th>phos</th>\n",
       "      <th>s_cotinine</th>\n",
       "      <th>chloride</th>\n",
       "      <th>t_bilirubin</th>\n",
       "      <th>bmi_group</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bp_group</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>11.77</td>\n",
       "      <td>118.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.654</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>172.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.221</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>168.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.011</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3.74</td>\n",
       "      <td>144.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>16.300</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>104.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>212.000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>5.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.63</td>\n",
       "      <td>185.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.011</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.40</td>\n",
       "      <td>166.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>75.6</td>\n",
       "      <td>163.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>7.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>187.41</td>\n",
       "      <td>176.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>33.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.011</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>6.5</td>\n",
       "      <td>126.0</td>\n",
       "      <td>11.43</td>\n",
       "      <td>171.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.269</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>6.2</td>\n",
       "      <td>103.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>45.2</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.035</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3260 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a1c  glucose  alb_cr_ratio  t_chol  trigs  platelets  grip_strength  \\\n",
       "0     5.0     82.0         11.77   118.0   54.0      157.0           50.3   \n",
       "1     5.2     81.0          2.37   172.0   83.0      226.0           90.1   \n",
       "2     5.1     87.0          3.73   168.0  256.0      266.0           72.7   \n",
       "3     5.1     91.0          3.74   144.0   57.0      206.0           86.6   \n",
       "4     6.0     89.0          3.13   104.0   70.0      306.0           94.4   \n",
       "...   ...      ...           ...     ...    ...        ...            ...   \n",
       "5196  5.8     98.0         49.63   185.0   80.0      178.0           47.2   \n",
       "5197  6.0    100.0          9.40   166.0  105.0      189.0           75.6   \n",
       "5199  7.0    175.0        187.41   176.0  104.0      273.0           33.1   \n",
       "5201  6.5    126.0         11.43   171.0  130.0      205.0           65.2   \n",
       "5204  6.2    103.0          3.92   181.0   67.0      213.0           45.2   \n",
       "\n",
       "        ldh    cr  alk_phos  ...  hypertension  gen_health  phos  s_cotinine  \\\n",
       "0      75.0  0.44      44.0  ...           2.0         3.0   4.2       0.654   \n",
       "1     137.0  0.81     112.0  ...           2.0         3.0   3.8       0.221   \n",
       "2     112.0  0.82     103.0  ...           2.0         3.0   4.4       0.011   \n",
       "3      87.0  0.73      65.0  ...           2.0         4.0   4.2      16.300   \n",
       "4     104.0  1.07      55.0  ...           2.0         2.0   4.3     212.000   \n",
       "...     ...   ...       ...  ...           ...         ...   ...         ...   \n",
       "5196  127.0  0.55      42.0  ...           1.0         3.0   3.7       0.011   \n",
       "5197  163.0  1.15      49.0  ...           2.0         3.0   3.0       0.011   \n",
       "5199  155.0  0.92      84.0  ...           1.0         3.0   3.9       0.011   \n",
       "5201  119.0  0.73      50.0  ...           2.0         2.0   3.1       0.269   \n",
       "5204  138.0  0.95      80.0  ...           1.0         3.0   3.5       0.035   \n",
       "\n",
       "      chloride  t_bilirubin  bmi_group  age_group  bp_group  diabetes  \n",
       "0        105.0          0.8          0          1         0       1.0  \n",
       "1        102.0          1.2          1          1         0       1.0  \n",
       "2        104.0          0.4          1          1         0       1.0  \n",
       "3        102.0          0.9          0          1         0       1.0  \n",
       "4        104.0          0.8          2          1         2       1.0  \n",
       "...        ...          ...        ...        ...       ...       ...  \n",
       "5196     103.0          0.9          0          4         2       1.0  \n",
       "5197     105.0          1.0          1          4         2       1.0  \n",
       "5199     102.0          0.5          2          4         2       0.0  \n",
       "5201     107.0          0.7          2          4         2       0.0  \n",
       "5204     103.0          1.0          2          4         1       0.0  \n",
       "\n",
       "[3260 rows x 35 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1c</th>\n",
       "      <th>glucose</th>\n",
       "      <th>alb_cr_ratio</th>\n",
       "      <th>t_chol</th>\n",
       "      <th>trigs</th>\n",
       "      <th>platelets</th>\n",
       "      <th>grip_strength</th>\n",
       "      <th>ldh</th>\n",
       "      <th>cr</th>\n",
       "      <th>alk_phos</th>\n",
       "      <th>...</th>\n",
       "      <th>iron</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>gen_health</th>\n",
       "      <th>phos</th>\n",
       "      <th>s_cotinine</th>\n",
       "      <th>chloride</th>\n",
       "      <th>t_bilirubin</th>\n",
       "      <th>bmi_group</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bp_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>11.77</td>\n",
       "      <td>118.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.654</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>172.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>119.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.221</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>168.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.011</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3.74</td>\n",
       "      <td>144.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>16.300</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>104.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>121.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>212.000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a1c  glucose  alb_cr_ratio  t_chol  trigs  platelets  grip_strength    ldh  \\\n",
       "0  5.0     82.0         11.77   118.0   54.0      157.0           50.3   75.0   \n",
       "1  5.2     81.0          2.37   172.0   83.0      226.0           90.1  137.0   \n",
       "2  5.1     87.0          3.73   168.0  256.0      266.0           72.7  112.0   \n",
       "3  5.1     91.0          3.74   144.0   57.0      206.0           86.6   87.0   \n",
       "4  6.0     89.0          3.13   104.0   70.0      306.0           94.4  104.0   \n",
       "\n",
       "     cr  alk_phos  ...   iron  hypertension  gen_health  phos  s_cotinine  \\\n",
       "0  0.44      44.0  ...  165.0           2.0         3.0   4.2       0.654   \n",
       "1  0.81     112.0  ...  119.0           2.0         3.0   3.8       0.221   \n",
       "2  0.82     103.0  ...   68.0           2.0         3.0   4.4       0.011   \n",
       "3  0.73      65.0  ...   63.0           2.0         4.0   4.2      16.300   \n",
       "4  1.07      55.0  ...  121.0           2.0         2.0   4.3     212.000   \n",
       "\n",
       "   chloride  t_bilirubin  bmi_group  age_group  bp_group  \n",
       "0     105.0          0.8          0          1         0  \n",
       "1     102.0          1.2          1          1         0  \n",
       "2     104.0          0.4          1          1         0  \n",
       "3     102.0          0.9          0          1         0  \n",
       "4     104.0          0.8          2          1         2  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = diab_df.drop(\"diabetes\", axis=1) \n",
    "feature_names = data.columns \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = diab_df[\"diabetes\"].values.reshape(-1, 1)\n",
    "target_names = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of variables is: 34\n"
     ]
    }
   ],
   "source": [
    "numb_variables =len(feature_names.tolist())\n",
    "print(f\"The total number of variables is: {numb_variables}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###MY CODE STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULLTIPLE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train_scaled)\n",
    "plt.scatter(model.predict(X_train_scaled), model.predict(X_train_scaled) - y_train_scaled, c=\"blue\", label=\"Training Data\")\n",
    "plt.scatter(model.predict(X_test_scaled), model.predict(X_test_scaled) - y_test_scaled, c=\"orange\", label=\"Testing Data\")\n",
    "plt.legend()\n",
    "plt.hlines(y=0, xmin=y_test_scaled.min(), xmax=y_test_scaled.max())\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.6689199878963539, R2: 0.28500183491349296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions = model.predict(X_test_scaled)\n",
    "MSE = mean_squared_error(y_test_scaled, predictions)\n",
    "r2 = model.score(X_test_scaled, y_test_scaled)\n",
    "\n",
    "print(f\"MSE: {MSE}, R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8601226993865031"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier() #model\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9251533742331288"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2171244923073894, 'a1c'),\n",
       " (0.14159017464662857, 'glucose'),\n",
       " (0.04033563916099368, 'alb_cr_ratio'),\n",
       " (0.027941550329338944, 't_chol'),\n",
       " (0.026653750273326317, 'platelets'),\n",
       " (0.02624347887169418, 'trigs'),\n",
       " (0.025297052120388457, 'ldh'),\n",
       " (0.02506681162018428, 'cr'),\n",
       " (0.02384038065560702, 'potassium'),\n",
       " (0.023437831286603792, 'grip_strength'),\n",
       " (0.023077049011410763, 'u_acid'),\n",
       " (0.023000820344757207, 'alk_phos'),\n",
       " (0.022390625834976207, 'cpk'),\n",
       " (0.021553008392466353, 'hdl'),\n",
       " (0.021350570647872612, 'bun'),\n",
       " (0.02045068040367984, 'wbc'),\n",
       " (0.019422131241907767, 'alt'),\n",
       " (0.019293107139802312, 'hct'),\n",
       " (0.01919948459506571, 'ast'),\n",
       " (0.018989805747332028, 'ca'),\n",
       " (0.018607723760869027, 'chloride'),\n",
       " (0.018179912827706878, 'gen_health'),\n",
       " (0.01795007565832322, 't_protein'),\n",
       " (0.017857817995464993, 'glob'),\n",
       " (0.01763448577599942, 'iron'),\n",
       " (0.017425142103149462, 'hgb'),\n",
       " (0.01688265752120986, 'phos'),\n",
       " (0.016817490750511317, 's_cotinine'),\n",
       " (0.016776521921147675, 'sodium'),\n",
       " (0.015590925192449699, 't_bilirubin'),\n",
       " (0.014078912133611388, 'age_group'),\n",
       " (0.012364488970611832, 'hypertension'),\n",
       " (0.008843678593653955, 'bmi_group'),\n",
       " (0.004731722163865794, 'bp_group')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf.feature_importances_, feature_names), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 1, Train/Test Score: 1.000/0.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3, Train/Test Score: 0.915/0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 5, Train/Test Score: 0.901/0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 7, Train/Test Score: 0.896/0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 9, Train/Test Score: 0.894/0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 11, Train/Test Score: 0.892/0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 13, Train/Test Score: 0.892/0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 15, Train/Test Score: 0.889/0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 17, Train/Test Score: 0.889/0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 19, Train/Test Score: 0.887/0.890\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=11 Test Acc: 0.893\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=11)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print('k=11 Test Acc: %.3f' % knn.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine linear classifier\n",
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] C=1, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0001, score=0.910, total=   0.5s\n",
      "[CV] C=1, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0001, score=0.928, total=   0.2s\n",
      "[CV] C=1, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n",
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0001, score=0.898, total=   0.4s\n",
      "[CV] C=1, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0001, score=0.910, total=   0.3s\n",
      "[CV] C=1, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0001, score=0.910, total=   0.4s\n",
      "[CV] C=1, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0005, score=0.910, total=   0.4s\n",
      "[CV] C=1, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0005, score=0.928, total=   0.7s\n",
      "[CV] C=1, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0005, score=0.898, total=   0.3s\n",
      "[CV] C=1, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0005, score=0.910, total=   0.4s\n",
      "[CV] C=1, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1, gamma=0.0005, score=0.910, total=   0.2s\n",
      "[CV] C=1, gamma=0.001 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.001, score=0.910, total=   0.4s\n",
      "[CV] C=1, gamma=0.001 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.001, score=0.928, total=   0.4s\n",
      "[CV] C=1, gamma=0.001 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.001, score=0.898, total=   0.3s\n",
      "[CV] C=1, gamma=0.001 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.001, score=0.910, total=   0.4s\n",
      "[CV] C=1, gamma=0.001 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.001, score=0.910, total=   0.2s\n",
      "[CV] C=1, gamma=0.005 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.005, score=0.910, total=   0.4s\n",
      "[CV] C=1, gamma=0.005 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.005, score=0.928, total=   0.3s\n",
      "[CV] C=1, gamma=0.005 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.005, score=0.898, total=   0.6s\n",
      "[CV] C=1, gamma=0.005 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.005, score=0.910, total=   0.5s\n",
      "[CV] C=1, gamma=0.005 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... C=1, gamma=0.005, score=0.910, total=   0.2s\n",
      "[CV] C=5, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0001, score=0.910, total=   1.6s\n",
      "[CV] C=5, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0001, score=0.928, total=   3.1s\n",
      "[CV] C=5, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0001, score=0.898, total=   1.5s\n",
      "[CV] C=5, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0001, score=0.912, total=   2.1s\n",
      "[CV] C=5, gamma=0.0001 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0001, score=0.910, total=   1.5s\n",
      "[CV] C=5, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0005, score=0.910, total=   0.9s\n",
      "[CV] C=5, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0005, score=0.928, total=   2.0s\n",
      "[CV] C=5, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0005, score=0.898, total=   1.1s\n",
      "[CV] C=5, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=5, gamma=0.0005, score=0.912, total=   1.7s\n",
      "[CV] C=5, gamma=0.0005 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5287c59abe55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model using the grid search estimator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This will take the SVC model and try each combination of parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 253\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model using the grid search estimator. \n",
    "# This will take the SVC model and try each combination of parameters\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # List the best parameters for this dataset\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best score\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"one\", \"two\",\"three\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_data = [[new data inserted here]]\n",
    "#predicted_class = knn.predict(new_data)\n",
    "#print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEPLEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(units=20, activation='relu', input_dim=34))\n",
    "deep_model.add(Dense(units=10, activation='relu'))\n",
    "deep_model.add(Dense(units=20, activation='relu'))\n",
    "deep_model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2445, 3)\n",
      "_----------\n",
      "(2445, 34)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_categorical.shape)\n",
    "print(\"_----------\")\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 - 0s - loss: 0.7378 - accuracy: 0.8356\n",
      "Epoch 2/200\n",
      "77/77 - 0s - loss: 0.4518 - accuracy: 0.8585\n",
      "Epoch 3/200\n",
      "77/77 - 0s - loss: 0.3631 - accuracy: 0.8851\n",
      "Epoch 4/200\n",
      "77/77 - 0s - loss: 0.3235 - accuracy: 0.8990\n",
      "Epoch 5/200\n",
      "77/77 - 0s - loss: 0.3007 - accuracy: 0.9080\n",
      "Epoch 6/200\n",
      "77/77 - 0s - loss: 0.2880 - accuracy: 0.9133\n",
      "Epoch 7/200\n",
      "77/77 - 0s - loss: 0.2788 - accuracy: 0.9162\n",
      "Epoch 8/200\n",
      "77/77 - 0s - loss: 0.2706 - accuracy: 0.9166\n",
      "Epoch 9/200\n",
      "77/77 - 0s - loss: 0.2633 - accuracy: 0.9190\n",
      "Epoch 10/200\n",
      "77/77 - 0s - loss: 0.2574 - accuracy: 0.9186\n",
      "Epoch 11/200\n",
      "77/77 - 0s - loss: 0.2515 - accuracy: 0.9215\n",
      "Epoch 12/200\n",
      "77/77 - 0s - loss: 0.2469 - accuracy: 0.9198\n",
      "Epoch 13/200\n",
      "77/77 - 0s - loss: 0.2415 - accuracy: 0.9260\n",
      "Epoch 14/200\n",
      "77/77 - 0s - loss: 0.2377 - accuracy: 0.9239\n",
      "Epoch 15/200\n",
      "77/77 - 0s - loss: 0.2329 - accuracy: 0.9272\n",
      "Epoch 16/200\n",
      "77/77 - 0s - loss: 0.2291 - accuracy: 0.9276\n",
      "Epoch 17/200\n",
      "77/77 - 0s - loss: 0.2256 - accuracy: 0.9301\n",
      "Epoch 18/200\n",
      "77/77 - 0s - loss: 0.2218 - accuracy: 0.9305\n",
      "Epoch 19/200\n",
      "77/77 - 0s - loss: 0.2185 - accuracy: 0.9297\n",
      "Epoch 20/200\n",
      "77/77 - 0s - loss: 0.2145 - accuracy: 0.9305\n",
      "Epoch 21/200\n",
      "77/77 - 0s - loss: 0.2110 - accuracy: 0.9358\n",
      "Epoch 22/200\n",
      "77/77 - 0s - loss: 0.2080 - accuracy: 0.9346\n",
      "Epoch 23/200\n",
      "77/77 - 0s - loss: 0.2040 - accuracy: 0.9366\n",
      "Epoch 24/200\n",
      "77/77 - 0s - loss: 0.2003 - accuracy: 0.9387\n",
      "Epoch 25/200\n",
      "77/77 - 0s - loss: 0.1965 - accuracy: 0.9427\n",
      "Epoch 26/200\n",
      "77/77 - 0s - loss: 0.1957 - accuracy: 0.9374\n",
      "Epoch 27/200\n",
      "77/77 - 0s - loss: 0.1911 - accuracy: 0.9407\n",
      "Epoch 28/200\n",
      "77/77 - 0s - loss: 0.1874 - accuracy: 0.9423\n",
      "Epoch 29/200\n",
      "77/77 - 0s - loss: 0.1851 - accuracy: 0.9431\n",
      "Epoch 30/200\n",
      "77/77 - 0s - loss: 0.1805 - accuracy: 0.9419\n",
      "Epoch 31/200\n",
      "77/77 - 0s - loss: 0.1774 - accuracy: 0.9456\n",
      "Epoch 32/200\n",
      "77/77 - 0s - loss: 0.1743 - accuracy: 0.9456\n",
      "Epoch 33/200\n",
      "77/77 - 0s - loss: 0.1723 - accuracy: 0.9460\n",
      "Epoch 34/200\n",
      "77/77 - 0s - loss: 0.1687 - accuracy: 0.9452\n",
      "Epoch 35/200\n",
      "77/77 - 0s - loss: 0.1653 - accuracy: 0.9481\n",
      "Epoch 36/200\n",
      "77/77 - 0s - loss: 0.1624 - accuracy: 0.9481\n",
      "Epoch 37/200\n",
      "77/77 - 0s - loss: 0.1595 - accuracy: 0.9489\n",
      "Epoch 38/200\n",
      "77/77 - 0s - loss: 0.1569 - accuracy: 0.9493\n",
      "Epoch 39/200\n",
      "77/77 - 0s - loss: 0.1572 - accuracy: 0.9505\n",
      "Epoch 40/200\n",
      "77/77 - 0s - loss: 0.1523 - accuracy: 0.9509\n",
      "Epoch 41/200\n",
      "77/77 - 0s - loss: 0.1500 - accuracy: 0.9542\n",
      "Epoch 42/200\n",
      "77/77 - 0s - loss: 0.1489 - accuracy: 0.9530\n",
      "Epoch 43/200\n",
      "77/77 - 0s - loss: 0.1462 - accuracy: 0.9558\n",
      "Epoch 44/200\n",
      "77/77 - 0s - loss: 0.1432 - accuracy: 0.9546\n",
      "Epoch 45/200\n",
      "77/77 - 0s - loss: 0.1410 - accuracy: 0.9550\n",
      "Epoch 46/200\n",
      "77/77 - 0s - loss: 0.1404 - accuracy: 0.9554\n",
      "Epoch 47/200\n",
      "77/77 - 0s - loss: 0.1359 - accuracy: 0.9558\n",
      "Epoch 48/200\n",
      "77/77 - 0s - loss: 0.1344 - accuracy: 0.9575\n",
      "Epoch 49/200\n",
      "77/77 - 1s - loss: 0.1313 - accuracy: 0.9579\n",
      "Epoch 50/200\n",
      "77/77 - 0s - loss: 0.1291 - accuracy: 0.9583\n",
      "Epoch 51/200\n",
      "77/77 - 0s - loss: 0.1279 - accuracy: 0.9587\n",
      "Epoch 52/200\n",
      "77/77 - 0s - loss: 0.1260 - accuracy: 0.9595\n",
      "Epoch 53/200\n",
      "77/77 - 0s - loss: 0.1247 - accuracy: 0.9587\n",
      "Epoch 54/200\n",
      "77/77 - 0s - loss: 0.1205 - accuracy: 0.9607\n",
      "Epoch 55/200\n",
      "77/77 - 0s - loss: 0.1193 - accuracy: 0.9624\n",
      "Epoch 56/200\n",
      "77/77 - 0s - loss: 0.1177 - accuracy: 0.9603\n",
      "Epoch 57/200\n",
      "77/77 - 0s - loss: 0.1155 - accuracy: 0.9616\n",
      "Epoch 58/200\n",
      "77/77 - 0s - loss: 0.1132 - accuracy: 0.9644\n",
      "Epoch 59/200\n",
      "77/77 - 0s - loss: 0.1100 - accuracy: 0.9640\n",
      "Epoch 60/200\n",
      "77/77 - 0s - loss: 0.1171 - accuracy: 0.9607\n",
      "Epoch 61/200\n",
      "77/77 - 0s - loss: 0.1105 - accuracy: 0.9632\n",
      "Epoch 62/200\n",
      "77/77 - 0s - loss: 0.1064 - accuracy: 0.9652\n",
      "Epoch 63/200\n",
      "77/77 - 0s - loss: 0.1031 - accuracy: 0.9665\n",
      "Epoch 64/200\n",
      "77/77 - 0s - loss: 0.1017 - accuracy: 0.9677\n",
      "Epoch 65/200\n",
      "77/77 - 0s - loss: 0.1006 - accuracy: 0.9665\n",
      "Epoch 66/200\n",
      "77/77 - 0s - loss: 0.0972 - accuracy: 0.9685\n",
      "Epoch 67/200\n",
      "77/77 - 0s - loss: 0.0969 - accuracy: 0.9681\n",
      "Epoch 68/200\n",
      "77/77 - 0s - loss: 0.0955 - accuracy: 0.9685\n",
      "Epoch 69/200\n",
      "77/77 - 0s - loss: 0.0952 - accuracy: 0.9661\n",
      "Epoch 70/200\n",
      "77/77 - 0s - loss: 0.0921 - accuracy: 0.9701\n",
      "Epoch 71/200\n",
      "77/77 - 0s - loss: 0.0893 - accuracy: 0.9693\n",
      "Epoch 72/200\n",
      "77/77 - 0s - loss: 0.0870 - accuracy: 0.9706\n",
      "Epoch 73/200\n",
      "77/77 - 0s - loss: 0.0857 - accuracy: 0.9714\n",
      "Epoch 74/200\n",
      "77/77 - 0s - loss: 0.0852 - accuracy: 0.9718\n",
      "Epoch 75/200\n",
      "77/77 - 0s - loss: 0.0824 - accuracy: 0.9722\n",
      "Epoch 76/200\n",
      "77/77 - 0s - loss: 0.0818 - accuracy: 0.9718\n",
      "Epoch 77/200\n",
      "77/77 - 0s - loss: 0.0802 - accuracy: 0.9726\n",
      "Epoch 78/200\n",
      "77/77 - 0s - loss: 0.0780 - accuracy: 0.9726\n",
      "Epoch 79/200\n",
      "77/77 - 0s - loss: 0.0772 - accuracy: 0.9738\n",
      "Epoch 80/200\n",
      "77/77 - 0s - loss: 0.0751 - accuracy: 0.9742\n",
      "Epoch 81/200\n",
      "77/77 - 0s - loss: 0.0739 - accuracy: 0.9746\n",
      "Epoch 82/200\n",
      "77/77 - 0s - loss: 0.0720 - accuracy: 0.9738\n",
      "Epoch 83/200\n",
      "77/77 - 0s - loss: 0.0694 - accuracy: 0.9746\n",
      "Epoch 84/200\n",
      "77/77 - 0s - loss: 0.0682 - accuracy: 0.9759\n",
      "Epoch 85/200\n",
      "77/77 - 0s - loss: 0.0688 - accuracy: 0.9755\n",
      "Epoch 86/200\n",
      "77/77 - 0s - loss: 0.0658 - accuracy: 0.9783\n",
      "Epoch 87/200\n",
      "77/77 - 0s - loss: 0.0660 - accuracy: 0.9791\n",
      "Epoch 88/200\n",
      "77/77 - 0s - loss: 0.0671 - accuracy: 0.9755\n",
      "Epoch 89/200\n",
      "77/77 - 0s - loss: 0.0651 - accuracy: 0.9779\n",
      "Epoch 90/200\n",
      "77/77 - 0s - loss: 0.0626 - accuracy: 0.9779\n",
      "Epoch 91/200\n",
      "77/77 - 0s - loss: 0.0587 - accuracy: 0.9804\n",
      "Epoch 92/200\n",
      "77/77 - 0s - loss: 0.0578 - accuracy: 0.9820\n",
      "Epoch 93/200\n",
      "77/77 - 0s - loss: 0.0552 - accuracy: 0.9828\n",
      "Epoch 94/200\n",
      "77/77 - 0s - loss: 0.0557 - accuracy: 0.9828\n",
      "Epoch 95/200\n",
      "77/77 - 0s - loss: 0.0543 - accuracy: 0.9824\n",
      "Epoch 96/200\n",
      "77/77 - 0s - loss: 0.0540 - accuracy: 0.9840\n",
      "Epoch 97/200\n",
      "77/77 - 0s - loss: 0.0521 - accuracy: 0.9832\n",
      "Epoch 98/200\n",
      "77/77 - 0s - loss: 0.0516 - accuracy: 0.9832\n",
      "Epoch 99/200\n",
      "77/77 - 0s - loss: 0.0510 - accuracy: 0.9849\n",
      "Epoch 100/200\n",
      "77/77 - 0s - loss: 0.0615 - accuracy: 0.9787\n",
      "Epoch 101/200\n",
      "77/77 - 0s - loss: 0.0642 - accuracy: 0.9791\n",
      "Epoch 102/200\n",
      "77/77 - 0s - loss: 0.0547 - accuracy: 0.9828\n",
      "Epoch 103/200\n",
      "77/77 - 0s - loss: 0.0496 - accuracy: 0.9845\n",
      "Epoch 104/200\n",
      "77/77 - 0s - loss: 0.0606 - accuracy: 0.9808\n",
      "Epoch 105/200\n",
      "77/77 - 0s - loss: 0.0477 - accuracy: 0.9869\n",
      "Epoch 106/200\n",
      "77/77 - 0s - loss: 0.0434 - accuracy: 0.9869\n",
      "Epoch 107/200\n",
      "77/77 - 0s - loss: 0.0428 - accuracy: 0.9869\n",
      "Epoch 108/200\n",
      "77/77 - 0s - loss: 0.0408 - accuracy: 0.9877\n",
      "Epoch 109/200\n",
      "77/77 - 0s - loss: 0.0410 - accuracy: 0.9877\n",
      "Epoch 110/200\n",
      "77/77 - 0s - loss: 0.0413 - accuracy: 0.9890\n",
      "Epoch 111/200\n",
      "77/77 - 0s - loss: 0.0384 - accuracy: 0.9877\n",
      "Epoch 112/200\n",
      "77/77 - 0s - loss: 0.0374 - accuracy: 0.9885\n",
      "Epoch 113/200\n",
      "77/77 - 0s - loss: 0.0382 - accuracy: 0.9890\n",
      "Epoch 114/200\n",
      "77/77 - 0s - loss: 0.0351 - accuracy: 0.9885\n",
      "Epoch 115/200\n",
      "77/77 - 0s - loss: 0.0340 - accuracy: 0.9894\n",
      "Epoch 116/200\n",
      "77/77 - 0s - loss: 0.0335 - accuracy: 0.9894\n",
      "Epoch 117/200\n",
      "77/77 - 0s - loss: 0.0322 - accuracy: 0.9890\n",
      "Epoch 118/200\n",
      "77/77 - 0s - loss: 0.0319 - accuracy: 0.9902\n",
      "Epoch 119/200\n",
      "77/77 - 0s - loss: 0.0328 - accuracy: 0.9894\n",
      "Epoch 120/200\n",
      "77/77 - 0s - loss: 0.0318 - accuracy: 0.9890\n",
      "Epoch 121/200\n",
      "77/77 - 0s - loss: 0.0321 - accuracy: 0.9902\n",
      "Epoch 122/200\n",
      "77/77 - 0s - loss: 0.0308 - accuracy: 0.9902\n",
      "Epoch 123/200\n",
      "77/77 - 0s - loss: 0.0284 - accuracy: 0.9906\n",
      "Epoch 124/200\n",
      "77/77 - 0s - loss: 0.0273 - accuracy: 0.9914\n",
      "Epoch 125/200\n",
      "77/77 - 0s - loss: 0.0284 - accuracy: 0.9910\n",
      "Epoch 126/200\n",
      "77/77 - 0s - loss: 0.0277 - accuracy: 0.9914\n",
      "Epoch 127/200\n",
      "77/77 - 0s - loss: 0.0254 - accuracy: 0.9922\n",
      "Epoch 128/200\n",
      "77/77 - 0s - loss: 0.0246 - accuracy: 0.9935\n",
      "Epoch 129/200\n",
      "77/77 - 0s - loss: 0.0253 - accuracy: 0.9918\n",
      "Epoch 130/200\n",
      "77/77 - 0s - loss: 0.0229 - accuracy: 0.9939\n",
      "Epoch 131/200\n",
      "77/77 - 0s - loss: 0.0230 - accuracy: 0.9922\n",
      "Epoch 132/200\n",
      "77/77 - 0s - loss: 0.0241 - accuracy: 0.9926\n",
      "Epoch 133/200\n",
      "77/77 - 0s - loss: 0.0255 - accuracy: 0.9918\n",
      "Epoch 134/200\n",
      "77/77 - 0s - loss: 0.0236 - accuracy: 0.9935\n",
      "Epoch 135/200\n",
      "77/77 - 0s - loss: 0.0210 - accuracy: 0.9947\n",
      "Epoch 136/200\n",
      "77/77 - 0s - loss: 0.0198 - accuracy: 0.9935\n",
      "Epoch 137/200\n",
      "77/77 - 0s - loss: 0.0186 - accuracy: 0.9939\n",
      "Epoch 138/200\n",
      "77/77 - 0s - loss: 0.0188 - accuracy: 0.9943\n",
      "Epoch 139/200\n",
      "77/77 - 0s - loss: 0.0178 - accuracy: 0.9939\n",
      "Epoch 140/200\n",
      "77/77 - 0s - loss: 0.0189 - accuracy: 0.9959\n",
      "Epoch 141/200\n",
      "77/77 - 0s - loss: 0.0168 - accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "77/77 - 0s - loss: 0.0209 - accuracy: 0.9930\n",
      "Epoch 143/200\n",
      "77/77 - 0s - loss: 0.0629 - accuracy: 0.9832\n",
      "Epoch 144/200\n",
      "77/77 - 0s - loss: 0.0418 - accuracy: 0.9869\n",
      "Epoch 145/200\n",
      "77/77 - 0s - loss: 0.0272 - accuracy: 0.9914\n",
      "Epoch 146/200\n",
      "77/77 - 0s - loss: 0.0193 - accuracy: 0.9947\n",
      "Epoch 147/200\n",
      "77/77 - 0s - loss: 0.0168 - accuracy: 0.9955\n",
      "Epoch 148/200\n",
      "77/77 - 0s - loss: 0.0157 - accuracy: 0.9963\n",
      "Epoch 149/200\n",
      "77/77 - 0s - loss: 0.0144 - accuracy: 0.9971\n",
      "Epoch 150/200\n",
      "77/77 - 0s - loss: 0.0141 - accuracy: 0.9975\n",
      "Epoch 151/200\n",
      "77/77 - 0s - loss: 0.0135 - accuracy: 0.9980\n",
      "Epoch 152/200\n",
      "77/77 - 0s - loss: 0.0133 - accuracy: 0.9980\n",
      "Epoch 153/200\n",
      "77/77 - 0s - loss: 0.0126 - accuracy: 0.9984\n",
      "Epoch 154/200\n",
      "77/77 - 0s - loss: 0.0122 - accuracy: 0.9984\n",
      "Epoch 155/200\n",
      "77/77 - 0s - loss: 0.0122 - accuracy: 0.9984\n",
      "Epoch 156/200\n",
      "77/77 - 0s - loss: 0.0116 - accuracy: 0.9988\n",
      "Epoch 157/200\n",
      "77/77 - 0s - loss: 0.0111 - accuracy: 0.9984\n",
      "Epoch 158/200\n",
      "77/77 - 0s - loss: 0.0109 - accuracy: 0.9984\n",
      "Epoch 159/200\n",
      "77/77 - 0s - loss: 0.0104 - accuracy: 0.9988\n",
      "Epoch 160/200\n",
      "77/77 - 0s - loss: 0.0104 - accuracy: 0.9988\n",
      "Epoch 161/200\n",
      "77/77 - 0s - loss: 0.0100 - accuracy: 0.9984\n",
      "Epoch 162/200\n",
      "77/77 - 0s - loss: 0.0098 - accuracy: 0.9992\n",
      "Epoch 163/200\n",
      "77/77 - 0s - loss: 0.0096 - accuracy: 0.9992\n",
      "Epoch 164/200\n",
      "77/77 - 0s - loss: 0.0095 - accuracy: 0.9992\n",
      "Epoch 165/200\n",
      "77/77 - 0s - loss: 0.0089 - accuracy: 0.9988\n",
      "Epoch 166/200\n",
      "77/77 - 0s - loss: 0.0088 - accuracy: 0.9988\n",
      "Epoch 167/200\n",
      "77/77 - 0s - loss: 0.0089 - accuracy: 0.9988\n",
      "Epoch 168/200\n",
      "77/77 - 0s - loss: 0.0084 - accuracy: 0.9988\n",
      "Epoch 169/200\n",
      "77/77 - 0s - loss: 0.0084 - accuracy: 0.9988\n",
      "Epoch 170/200\n",
      "77/77 - 0s - loss: 0.0084 - accuracy: 0.9984\n",
      "Epoch 171/200\n",
      "77/77 - 0s - loss: 0.0077 - accuracy: 0.9992\n",
      "Epoch 172/200\n",
      "77/77 - 0s - loss: 0.0071 - accuracy: 0.9992\n",
      "Epoch 173/200\n",
      "77/77 - 0s - loss: 0.0069 - accuracy: 0.9992\n",
      "Epoch 174/200\n",
      "77/77 - 0s - loss: 0.0067 - accuracy: 0.9992\n",
      "Epoch 175/200\n",
      "77/77 - 0s - loss: 0.0063 - accuracy: 0.9992\n",
      "Epoch 176/200\n",
      "77/77 - 0s - loss: 0.0062 - accuracy: 0.9992\n",
      "Epoch 177/200\n",
      "77/77 - 0s - loss: 0.0061 - accuracy: 0.9992\n",
      "Epoch 178/200\n",
      "77/77 - 0s - loss: 0.0058 - accuracy: 0.9996\n",
      "Epoch 179/200\n",
      "77/77 - 0s - loss: 0.0065 - accuracy: 0.9992\n",
      "Epoch 180/200\n",
      "77/77 - 0s - loss: 0.0671 - accuracy: 0.9828\n",
      "Epoch 181/200\n",
      "77/77 - 0s - loss: 0.0675 - accuracy: 0.9791\n",
      "Epoch 182/200\n",
      "77/77 - 0s - loss: 0.0289 - accuracy: 0.9906\n",
      "Epoch 183/200\n",
      "77/77 - 0s - loss: 0.0407 - accuracy: 0.9947\n",
      "Epoch 184/200\n",
      "77/77 - 0s - loss: 0.0173 - accuracy: 0.9967\n",
      "Epoch 185/200\n",
      "77/77 - 0s - loss: 0.0133 - accuracy: 0.9975\n",
      "Epoch 186/200\n",
      "77/77 - 0s - loss: 0.0116 - accuracy: 0.9988\n",
      "Epoch 187/200\n",
      "77/77 - 0s - loss: 0.0080 - accuracy: 0.9992\n",
      "Epoch 188/200\n",
      "77/77 - 0s - loss: 0.0082 - accuracy: 0.9988\n",
      "Epoch 189/200\n",
      "77/77 - 0s - loss: 0.0073 - accuracy: 0.9992\n",
      "Epoch 190/200\n",
      "77/77 - 0s - loss: 0.0068 - accuracy: 0.9992\n",
      "Epoch 191/200\n",
      "77/77 - 0s - loss: 0.0064 - accuracy: 0.9996\n",
      "Epoch 192/200\n",
      "77/77 - 0s - loss: 0.0063 - accuracy: 0.9992\n",
      "Epoch 193/200\n",
      "77/77 - 0s - loss: 0.0059 - accuracy: 0.9992\n",
      "Epoch 194/200\n",
      "77/77 - 0s - loss: 0.0058 - accuracy: 0.9992\n",
      "Epoch 195/200\n",
      "77/77 - 0s - loss: 0.0056 - accuracy: 0.9992\n",
      "Epoch 196/200\n",
      "77/77 - 0s - loss: 0.0055 - accuracy: 0.9996\n",
      "Epoch 197/200\n",
      "77/77 - 0s - loss: 0.0053 - accuracy: 0.9996\n",
      "Epoch 198/200\n",
      "77/77 - 0s - loss: 0.0052 - accuracy: 0.9996\n",
      "Epoch 199/200\n",
      "77/77 - 0s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "77/77 - 0s - loss: 0.0048 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec25a3fef0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 - 0s - loss: 1.8739 - accuracy: 0.8663\n",
      "Deep Neural Network - Loss: 1.8738971948623657, Accuracy: 0.8662576675415039\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = deep_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Deep Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acuracy 0.87\n",
    "#Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = deep_model.predict(X_test)\n",
    "                            \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "\n",
    "predictions_categorical = predictions.astype(int)\n",
    "\n",
    "predictions = np.argmax(predictions_categorical, axis=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 0.,\n",
       "       1., 1., 0., 1., 0., 2., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 2., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 2., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       2., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 2., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 2., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 2., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 2., 1., 1., 1., 1., 0., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = y_test\n",
    "labels = labels.ravel()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(815,)\n",
      "(815,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "cm = confusion_matrix(labels, predictions)\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(recall))\n",
    "print(np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.60      0.15      0.24        99\n",
      "     class 1       0.88      0.99      0.93       702\n",
      "     class 2       0.00      0.00      0.00        14\n",
      "\n",
      "    accuracy                           0.87       815\n",
      "   macro avg       0.49      0.38      0.39       815\n",
      "weighted avg       0.83      0.87      0.83       815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tajudeenadeyemi/opt/anaconda3/envs/PythonAdv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = labels\n",
    "y_pred = predictions\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP CLASS 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a1c</th>\n",
       "      <th>glucose</th>\n",
       "      <th>alb_cr_ratio</th>\n",
       "      <th>t_chol</th>\n",
       "      <th>trigs</th>\n",
       "      <th>platelets</th>\n",
       "      <th>grip_strength</th>\n",
       "      <th>ldh</th>\n",
       "      <th>cr</th>\n",
       "      <th>alk_phos</th>\n",
       "      <th>...</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>gen_health</th>\n",
       "      <th>phos</th>\n",
       "      <th>s_cotinine</th>\n",
       "      <th>chloride</th>\n",
       "      <th>t_bilirubin</th>\n",
       "      <th>bmi_group</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bp_group</th>\n",
       "      <th>diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>11.77</td>\n",
       "      <td>118.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>50.3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.654</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.2</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>172.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>112.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.221</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>87.0</td>\n",
       "      <td>3.73</td>\n",
       "      <td>168.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>72.7</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>103.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.011</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3.74</td>\n",
       "      <td>144.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>16.300</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>3.13</td>\n",
       "      <td>104.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>94.4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>212.000</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>5.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>49.63</td>\n",
       "      <td>185.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>47.2</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0.011</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>6.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.40</td>\n",
       "      <td>166.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>75.6</td>\n",
       "      <td>163.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>7.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>187.41</td>\n",
       "      <td>176.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>33.1</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.011</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>6.5</td>\n",
       "      <td>126.0</td>\n",
       "      <td>11.43</td>\n",
       "      <td>171.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.269</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>6.2</td>\n",
       "      <td>103.0</td>\n",
       "      <td>3.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>45.2</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.035</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3182 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a1c  glucose  alb_cr_ratio  t_chol  trigs  platelets  grip_strength  \\\n",
       "0     5.0     82.0         11.77   118.0   54.0      157.0           50.3   \n",
       "1     5.2     81.0          2.37   172.0   83.0      226.0           90.1   \n",
       "2     5.1     87.0          3.73   168.0  256.0      266.0           72.7   \n",
       "3     5.1     91.0          3.74   144.0   57.0      206.0           86.6   \n",
       "4     6.0     89.0          3.13   104.0   70.0      306.0           94.4   \n",
       "...   ...      ...           ...     ...    ...        ...            ...   \n",
       "5196  5.8     98.0         49.63   185.0   80.0      178.0           47.2   \n",
       "5197  6.0    100.0          9.40   166.0  105.0      189.0           75.6   \n",
       "5199  7.0    175.0        187.41   176.0  104.0      273.0           33.1   \n",
       "5201  6.5    126.0         11.43   171.0  130.0      205.0           65.2   \n",
       "5204  6.2    103.0          3.92   181.0   67.0      213.0           45.2   \n",
       "\n",
       "        ldh    cr  alk_phos  ...  hypertension  gen_health  phos  s_cotinine  \\\n",
       "0      75.0  0.44      44.0  ...           2.0         3.0   4.2       0.654   \n",
       "1     137.0  0.81     112.0  ...           2.0         3.0   3.8       0.221   \n",
       "2     112.0  0.82     103.0  ...           2.0         3.0   4.4       0.011   \n",
       "3      87.0  0.73      65.0  ...           2.0         4.0   4.2      16.300   \n",
       "4     104.0  1.07      55.0  ...           2.0         2.0   4.3     212.000   \n",
       "...     ...   ...       ...  ...           ...         ...   ...         ...   \n",
       "5196  127.0  0.55      42.0  ...           1.0         3.0   3.7       0.011   \n",
       "5197  163.0  1.15      49.0  ...           2.0         3.0   3.0       0.011   \n",
       "5199  155.0  0.92      84.0  ...           1.0         3.0   3.9       0.011   \n",
       "5201  119.0  0.73      50.0  ...           2.0         2.0   3.1       0.269   \n",
       "5204  138.0  0.95      80.0  ...           1.0         3.0   3.5       0.035   \n",
       "\n",
       "      chloride  t_bilirubin  bmi_group  age_group  bp_group  diabetes  \n",
       "0        105.0          0.8          0          1         0       1.0  \n",
       "1        102.0          1.2          1          1         0       1.0  \n",
       "2        104.0          0.4          1          1         0       1.0  \n",
       "3        102.0          0.9          0          1         0       1.0  \n",
       "4        104.0          0.8          2          1         2       1.0  \n",
       "...        ...          ...        ...        ...       ...       ...  \n",
       "5196     103.0          0.9          0          4         2       1.0  \n",
       "5197     105.0          1.0          1          4         2       1.0  \n",
       "5199     102.0          0.5          2          4         2       0.0  \n",
       "5201     107.0          0.7          2          4         2       0.0  \n",
       "5204     103.0          1.0          2          4         1       0.0  \n",
       "\n",
       "[3182 rows x 35 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab_df2 = diab_df[diab_df.diabetes != 2.0]\n",
    "diab_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_df4 = diab_df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    2792\n",
       "1.0     390\n",
       "Name: diabetes, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert DIAABETES VALUES 0->1 AND 1->0\n",
    "\n",
    "diab_df4['diabetes'] = diab_df4['diabetes'].replace([0,1],[1,0])\n",
    "diab_df4['diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = diab_df4.drop(\"diabetes\", axis=1) \n",
    "feature_names4 = data4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "target4 = diab_df4[\"diabetes\"].values.reshape(-1, 1)\n",
    "target_names4 = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of variables is: 34\n"
     ]
    }
   ],
   "source": [
    "numb_variables =len(feature_names.tolist())\n",
    "print(f\"The total number of variables is: {numb_variables}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data4, target4, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1432, 34)\n",
      "(1432, 1)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_test, y_test = oversample.fit_resample(X_test, y_test)\n",
    "\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEPLEARNING\n",
    "#y_predict = model.predict(X_test) \n",
    "#y_predict = (y_predict>0.25) # It will evaluate the logical expression y_predict>0.25 and return True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.metrics.Recall at 0x7fec290a9e48>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.metrics.Recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(units=102, activation='relu', input_dim=34))\n",
    "deep_model.add(Dense(units=68, activation='relu'))\n",
    "deep_model.add(Dense(units=68, activation='relu'))\n",
    "deep_model.add(Dense(units=68, activation='relu'))\n",
    "deep_model.add(Dense(units=68, activation='relu'))\n",
    "\n",
    "deep_model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[\"Recall\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/275\n",
      "75/75 - 0s - loss: 0.3339 - recall: 0.8726\n",
      "Epoch 2/275\n",
      "75/75 - 0s - loss: 0.2058 - recall: 0.9329\n",
      "Epoch 3/275\n",
      "75/75 - 0s - loss: 0.1749 - recall: 0.9363\n",
      "Epoch 4/275\n",
      "75/75 - 0s - loss: 0.1456 - recall: 0.9489\n",
      "Epoch 5/275\n",
      "75/75 - 0s - loss: 0.1293 - recall: 0.9535\n",
      "Epoch 6/275\n",
      "75/75 - 0s - loss: 0.1013 - recall: 0.9648\n",
      "Epoch 7/275\n",
      "75/75 - 0s - loss: 0.0780 - recall: 0.9753\n",
      "Epoch 8/275\n",
      "75/75 - 0s - loss: 0.0641 - recall: 0.9761\n",
      "Epoch 9/275\n",
      "75/75 - 0s - loss: 0.0502 - recall: 0.9807\n",
      "Epoch 10/275\n",
      "75/75 - 0s - loss: 0.0308 - recall: 0.9883\n",
      "Epoch 11/275\n",
      "75/75 - 0s - loss: 0.0233 - recall: 0.9920\n",
      "Epoch 12/275\n",
      "75/75 - 0s - loss: 0.0534 - recall: 0.9841\n",
      "Epoch 13/275\n",
      "75/75 - 0s - loss: 0.0379 - recall: 0.9904\n",
      "Epoch 14/275\n",
      "75/75 - 0s - loss: 0.0263 - recall: 0.9950\n",
      "Epoch 15/275\n",
      "75/75 - 0s - loss: 0.0072 - recall: 0.9992\n",
      "Epoch 16/275\n",
      "75/75 - 0s - loss: 0.0017 - recall: 1.0000\n",
      "Epoch 17/275\n",
      "75/75 - 0s - loss: 6.6578e-04 - recall: 1.0000\n",
      "Epoch 18/275\n",
      "75/75 - 0s - loss: 3.9936e-04 - recall: 1.0000\n",
      "Epoch 19/275\n",
      "75/75 - 0s - loss: 2.8327e-04 - recall: 1.0000\n",
      "Epoch 20/275\n",
      "75/75 - 0s - loss: 2.1171e-04 - recall: 1.0000\n",
      "Epoch 21/275\n",
      "75/75 - 0s - loss: 1.6682e-04 - recall: 1.0000\n",
      "Epoch 22/275\n",
      "75/75 - 0s - loss: 1.3497e-04 - recall: 1.0000\n",
      "Epoch 23/275\n",
      "75/75 - 0s - loss: 1.1222e-04 - recall: 1.0000\n",
      "Epoch 24/275\n",
      "75/75 - 0s - loss: 9.3438e-05 - recall: 1.0000\n",
      "Epoch 25/275\n",
      "75/75 - 0s - loss: 7.9555e-05 - recall: 1.0000\n",
      "Epoch 26/275\n",
      "75/75 - 0s - loss: 6.8166e-05 - recall: 1.0000\n",
      "Epoch 27/275\n",
      "75/75 - 0s - loss: 5.9670e-05 - recall: 1.0000\n",
      "Epoch 28/275\n",
      "75/75 - 0s - loss: 5.2219e-05 - recall: 1.0000\n",
      "Epoch 29/275\n",
      "75/75 - 0s - loss: 4.6130e-05 - recall: 1.0000\n",
      "Epoch 30/275\n",
      "75/75 - 0s - loss: 4.0964e-05 - recall: 1.0000\n",
      "Epoch 31/275\n",
      "75/75 - 0s - loss: 3.6658e-05 - recall: 1.0000\n",
      "Epoch 32/275\n",
      "75/75 - 0s - loss: 3.2949e-05 - recall: 1.0000\n",
      "Epoch 33/275\n",
      "75/75 - 0s - loss: 2.9683e-05 - recall: 1.0000\n",
      "Epoch 34/275\n",
      "75/75 - 0s - loss: 2.6895e-05 - recall: 1.0000\n",
      "Epoch 35/275\n",
      "75/75 - 0s - loss: 2.4557e-05 - recall: 1.0000\n",
      "Epoch 36/275\n",
      "75/75 - 0s - loss: 2.2402e-05 - recall: 1.0000\n",
      "Epoch 37/275\n",
      "75/75 - 0s - loss: 2.0432e-05 - recall: 1.0000\n",
      "Epoch 38/275\n",
      "75/75 - 0s - loss: 1.8761e-05 - recall: 1.0000\n",
      "Epoch 39/275\n",
      "75/75 - 0s - loss: 1.7280e-05 - recall: 1.0000\n",
      "Epoch 40/275\n",
      "75/75 - 0s - loss: 1.5931e-05 - recall: 1.0000\n",
      "Epoch 41/275\n",
      "75/75 - 0s - loss: 1.4720e-05 - recall: 1.0000\n",
      "Epoch 42/275\n",
      "75/75 - 0s - loss: 1.3635e-05 - recall: 1.0000\n",
      "Epoch 43/275\n",
      "75/75 - 0s - loss: 1.2656e-05 - recall: 1.0000\n",
      "Epoch 44/275\n",
      "75/75 - 0s - loss: 1.1753e-05 - recall: 1.0000\n",
      "Epoch 45/275\n",
      "75/75 - 0s - loss: 1.0912e-05 - recall: 1.0000\n",
      "Epoch 46/275\n",
      "75/75 - 0s - loss: 1.0171e-05 - recall: 1.0000\n",
      "Epoch 47/275\n",
      "75/75 - 0s - loss: 9.4883e-06 - recall: 1.0000\n",
      "Epoch 48/275\n",
      "75/75 - 0s - loss: 8.8641e-06 - recall: 1.0000\n",
      "Epoch 49/275\n",
      "75/75 - 0s - loss: 8.2890e-06 - recall: 1.0000\n",
      "Epoch 50/275\n",
      "75/75 - 0s - loss: 7.7652e-06 - recall: 1.0000\n",
      "Epoch 51/275\n",
      "75/75 - 0s - loss: 7.2804e-06 - recall: 1.0000\n",
      "Epoch 52/275\n",
      "75/75 - 0s - loss: 6.8255e-06 - recall: 1.0000\n",
      "Epoch 53/275\n",
      "75/75 - 0s - loss: 6.4120e-06 - recall: 1.0000\n",
      "Epoch 54/275\n",
      "75/75 - 0s - loss: 6.0314e-06 - recall: 1.0000\n",
      "Epoch 55/275\n",
      "75/75 - 0s - loss: 5.6685e-06 - recall: 1.0000\n",
      "Epoch 56/275\n",
      "75/75 - 0s - loss: 5.3406e-06 - recall: 1.0000\n",
      "Epoch 57/275\n",
      "75/75 - 0s - loss: 5.0285e-06 - recall: 1.0000\n",
      "Epoch 58/275\n",
      "75/75 - 0s - loss: 4.7467e-06 - recall: 1.0000\n",
      "Epoch 59/275\n",
      "75/75 - 0s - loss: 4.4803e-06 - recall: 1.0000\n",
      "Epoch 60/275\n",
      "75/75 - 0s - loss: 4.2226e-06 - recall: 1.0000\n",
      "Epoch 61/275\n",
      "75/75 - 0s - loss: 3.9890e-06 - recall: 1.0000\n",
      "Epoch 62/275\n",
      "75/75 - 0s - loss: 3.7681e-06 - recall: 1.0000\n",
      "Epoch 63/275\n",
      "75/75 - 0s - loss: 3.5601e-06 - recall: 1.0000\n",
      "Epoch 64/275\n",
      "75/75 - 0s - loss: 3.3659e-06 - recall: 1.0000\n",
      "Epoch 65/275\n",
      "75/75 - 0s - loss: 3.1907e-06 - recall: 1.0000\n",
      "Epoch 66/275\n",
      "75/75 - 0s - loss: 3.0212e-06 - recall: 1.0000\n",
      "Epoch 67/275\n",
      "75/75 - 0s - loss: 2.8595e-06 - recall: 1.0000\n",
      "Epoch 68/275\n",
      "75/75 - 0s - loss: 2.7132e-06 - recall: 1.0000\n",
      "Epoch 69/275\n",
      "75/75 - 0s - loss: 2.5728e-06 - recall: 1.0000\n",
      "Epoch 70/275\n",
      "75/75 - 0s - loss: 2.4368e-06 - recall: 1.0000\n",
      "Epoch 71/275\n",
      "75/75 - 0s - loss: 2.3119e-06 - recall: 1.0000\n",
      "Epoch 72/275\n",
      "75/75 - 0s - loss: 2.1953e-06 - recall: 1.0000\n",
      "Epoch 73/275\n",
      "75/75 - 0s - loss: 2.0830e-06 - recall: 1.0000\n",
      "Epoch 74/275\n",
      "75/75 - 0s - loss: 1.9798e-06 - recall: 1.0000\n",
      "Epoch 75/275\n",
      "75/75 - 0s - loss: 1.8787e-06 - recall: 1.0000\n",
      "Epoch 76/275\n",
      "75/75 - 0s - loss: 1.7862e-06 - recall: 1.0000\n",
      "Epoch 77/275\n",
      "75/75 - 0s - loss: 1.6990e-06 - recall: 1.0000\n",
      "Epoch 78/275\n",
      "75/75 - 0s - loss: 1.6146e-06 - recall: 1.0000\n",
      "Epoch 79/275\n",
      "75/75 - 0s - loss: 1.5347e-06 - recall: 1.0000\n",
      "Epoch 80/275\n",
      "75/75 - 0s - loss: 1.4609e-06 - recall: 1.0000\n",
      "Epoch 81/275\n",
      "75/75 - 0s - loss: 1.3888e-06 - recall: 1.0000\n",
      "Epoch 82/275\n",
      "75/75 - 0s - loss: 1.3229e-06 - recall: 1.0000\n",
      "Epoch 83/275\n",
      "75/75 - 0s - loss: 1.2598e-06 - recall: 1.0000\n",
      "Epoch 84/275\n",
      "75/75 - 0s - loss: 1.1972e-06 - recall: 1.0000\n",
      "Epoch 85/275\n",
      "75/75 - 0s - loss: 1.1408e-06 - recall: 1.0000\n",
      "Epoch 86/275\n",
      "75/75 - 0s - loss: 1.0854e-06 - recall: 1.0000\n",
      "Epoch 87/275\n",
      "75/75 - 0s - loss: 1.0337e-06 - recall: 1.0000\n",
      "Epoch 88/275\n",
      "75/75 - 0s - loss: 9.8469e-07 - recall: 1.0000\n",
      "Epoch 89/275\n",
      "75/75 - 0s - loss: 9.3802e-07 - recall: 1.0000\n",
      "Epoch 90/275\n",
      "75/75 - 0s - loss: 8.9468e-07 - recall: 1.0000\n",
      "Epoch 91/275\n",
      "75/75 - 0s - loss: 8.5161e-07 - recall: 1.0000\n",
      "Epoch 92/275\n",
      "75/75 - 0s - loss: 8.0995e-07 - recall: 1.0000\n",
      "Epoch 93/275\n",
      "75/75 - 0s - loss: 7.7177e-07 - recall: 1.0000\n",
      "Epoch 94/275\n",
      "75/75 - 0s - loss: 7.3608e-07 - recall: 1.0000\n",
      "Epoch 95/275\n",
      "75/75 - 0s - loss: 7.0222e-07 - recall: 1.0000\n",
      "Epoch 96/275\n",
      "75/75 - 0s - loss: 6.6910e-07 - recall: 1.0000\n",
      "Epoch 97/275\n",
      "75/75 - 0s - loss: 6.3867e-07 - recall: 1.0000\n",
      "Epoch 98/275\n",
      "75/75 - 0s - loss: 6.0836e-07 - recall: 1.0000\n",
      "Epoch 99/275\n",
      "75/75 - 0s - loss: 5.8083e-07 - recall: 1.0000\n",
      "Epoch 100/275\n",
      "75/75 - 0s - loss: 5.5407e-07 - recall: 1.0000\n",
      "Epoch 101/275\n",
      "75/75 - 0s - loss: 5.2734e-07 - recall: 1.0000\n",
      "Epoch 102/275\n",
      "75/75 - 0s - loss: 5.0351e-07 - recall: 1.0000\n",
      "Epoch 103/275\n",
      "75/75 - 0s - loss: 4.7944e-07 - recall: 1.0000\n",
      "Epoch 104/275\n",
      "75/75 - 0s - loss: 4.5706e-07 - recall: 1.0000\n",
      "Epoch 105/275\n",
      "75/75 - 0s - loss: 4.3592e-07 - recall: 1.0000\n",
      "Epoch 106/275\n",
      "75/75 - 0s - loss: 4.1620e-07 - recall: 1.0000\n",
      "Epoch 107/275\n",
      "75/75 - 0s - loss: 3.9766e-07 - recall: 1.0000\n",
      "Epoch 108/275\n",
      "75/75 - 0s - loss: 3.8010e-07 - recall: 1.0000\n",
      "Epoch 109/275\n",
      "75/75 - 0s - loss: 3.6235e-07 - recall: 1.0000\n",
      "Epoch 110/275\n",
      "75/75 - 0s - loss: 3.4610e-07 - recall: 1.0000\n",
      "Epoch 111/275\n",
      "75/75 - 0s - loss: 3.2989e-07 - recall: 1.0000\n",
      "Epoch 112/275\n",
      "75/75 - 0s - loss: 3.1515e-07 - recall: 1.0000\n",
      "Epoch 113/275\n",
      "75/75 - 0s - loss: 3.0094e-07 - recall: 1.0000\n",
      "Epoch 114/275\n",
      "75/75 - 0s - loss: 2.8690e-07 - recall: 1.0000\n",
      "Epoch 115/275\n",
      "75/75 - 0s - loss: 2.7401e-07 - recall: 1.0000\n",
      "Epoch 116/275\n",
      "75/75 - 0s - loss: 2.6154e-07 - recall: 1.0000\n",
      "Epoch 117/275\n",
      "75/75 - 0s - loss: 2.4945e-07 - recall: 1.0000\n",
      "Epoch 118/275\n",
      "75/75 - 0s - loss: 2.3818e-07 - recall: 1.0000\n",
      "Epoch 119/275\n",
      "75/75 - 0s - loss: 2.2753e-07 - recall: 1.0000\n",
      "Epoch 120/275\n",
      "75/75 - 0s - loss: 2.1737e-07 - recall: 1.0000\n",
      "Epoch 121/275\n",
      "75/75 - 0s - loss: 2.0749e-07 - recall: 1.0000\n",
      "Epoch 122/275\n",
      "75/75 - 0s - loss: 1.9811e-07 - recall: 1.0000\n",
      "Epoch 123/275\n",
      "75/75 - 0s - loss: 1.8917e-07 - recall: 1.0000\n",
      "Epoch 124/275\n",
      "75/75 - 0s - loss: 1.8083e-07 - recall: 1.0000\n",
      "Epoch 125/275\n",
      "75/75 - 0s - loss: 1.7229e-07 - recall: 1.0000\n",
      "Epoch 126/275\n",
      "75/75 - 0s - loss: 1.6465e-07 - recall: 1.0000\n",
      "Epoch 127/275\n",
      "75/75 - 0s - loss: 1.5761e-07 - recall: 1.0000\n",
      "Epoch 128/275\n",
      "75/75 - 0s - loss: 1.5064e-07 - recall: 1.0000\n",
      "Epoch 129/275\n",
      "75/75 - 0s - loss: 1.4353e-07 - recall: 1.0000\n",
      "Epoch 130/275\n",
      "75/75 - 0s - loss: 1.3676e-07 - recall: 1.0000\n",
      "Epoch 131/275\n",
      "75/75 - 0s - loss: 1.3059e-07 - recall: 1.0000\n",
      "Epoch 132/275\n",
      "75/75 - 0s - loss: 1.2463e-07 - recall: 1.0000\n",
      "Epoch 133/275\n",
      "75/75 - 0s - loss: 1.1911e-07 - recall: 1.0000\n",
      "Epoch 134/275\n",
      "75/75 - 0s - loss: 1.1403e-07 - recall: 1.0000\n",
      "Epoch 135/275\n",
      "75/75 - 0s - loss: 1.0839e-07 - recall: 1.0000\n",
      "Epoch 136/275\n",
      "75/75 - 0s - loss: 1.0375e-07 - recall: 1.0000\n",
      "Epoch 137/275\n",
      "75/75 - 0s - loss: 9.8763e-08 - recall: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/275\n",
      "75/75 - 0s - loss: 9.4441e-08 - recall: 1.0000\n",
      "Epoch 139/275\n",
      "75/75 - 0s - loss: 8.9919e-08 - recall: 1.0000\n",
      "Epoch 140/275\n",
      "75/75 - 0s - loss: 8.5985e-08 - recall: 1.0000\n",
      "Epoch 141/275\n",
      "75/75 - 0s - loss: 8.1963e-08 - recall: 1.0000\n",
      "Epoch 142/275\n",
      "75/75 - 0s - loss: 7.8553e-08 - recall: 1.0000\n",
      "Epoch 143/275\n",
      "75/75 - 0s - loss: 7.4668e-08 - recall: 1.0000\n",
      "Epoch 144/275\n",
      "75/75 - 0s - loss: 7.1283e-08 - recall: 1.0000\n",
      "Epoch 145/275\n",
      "75/75 - 0s - loss: 6.7886e-08 - recall: 1.0000\n",
      "Epoch 146/275\n",
      "75/75 - 0s - loss: 6.4701e-08 - recall: 1.0000\n",
      "Epoch 147/275\n",
      "75/75 - 0s - loss: 6.1741e-08 - recall: 1.0000\n",
      "Epoch 148/275\n",
      "75/75 - 0s - loss: 5.8868e-08 - recall: 1.0000\n",
      "Epoch 149/275\n",
      "75/75 - 0s - loss: 5.5758e-08 - recall: 1.0000\n",
      "Epoch 150/275\n",
      "75/75 - 0s - loss: 5.3297e-08 - recall: 1.0000\n",
      "Epoch 151/275\n",
      "75/75 - 0s - loss: 5.0799e-08 - recall: 1.0000\n",
      "Epoch 152/275\n",
      "75/75 - 0s - loss: 4.8376e-08 - recall: 1.0000\n",
      "Epoch 153/275\n",
      "75/75 - 0s - loss: 4.6015e-08 - recall: 1.0000\n",
      "Epoch 154/275\n",
      "75/75 - 0s - loss: 4.3779e-08 - recall: 1.0000\n",
      "Epoch 155/275\n",
      "75/75 - 0s - loss: 4.1668e-08 - recall: 1.0000\n",
      "Epoch 156/275\n",
      "75/75 - 0s - loss: 3.9670e-08 - recall: 1.0000\n",
      "Epoch 157/275\n",
      "75/75 - 0s - loss: 3.7784e-08 - recall: 1.0000\n",
      "Epoch 158/275\n",
      "75/75 - 0s - loss: 3.5860e-08 - recall: 1.0000\n",
      "Epoch 159/275\n",
      "75/75 - 0s - loss: 3.4236e-08 - recall: 1.0000\n",
      "Epoch 160/275\n",
      "75/75 - 0s - loss: 3.2688e-08 - recall: 1.0000\n",
      "Epoch 161/275\n",
      "75/75 - 0s - loss: 3.0964e-08 - recall: 1.0000\n",
      "Epoch 162/275\n",
      "75/75 - 0s - loss: 2.9478e-08 - recall: 1.0000\n",
      "Epoch 163/275\n",
      "75/75 - 0s - loss: 2.7954e-08 - recall: 1.0000\n",
      "Epoch 164/275\n",
      "75/75 - 0s - loss: 2.6567e-08 - recall: 1.0000\n",
      "Epoch 165/275\n",
      "75/75 - 0s - loss: 2.5206e-08 - recall: 1.0000\n",
      "Epoch 166/275\n",
      "75/75 - 0s - loss: 2.4032e-08 - recall: 1.0000\n",
      "Epoch 167/275\n",
      "75/75 - 0s - loss: 2.2908e-08 - recall: 1.0000\n",
      "Epoch 168/275\n",
      "75/75 - 0s - loss: 2.1721e-08 - recall: 1.0000\n",
      "Epoch 169/275\n",
      "75/75 - 0s - loss: 2.0734e-08 - recall: 1.0000\n",
      "Epoch 170/275\n",
      "75/75 - 0s - loss: 1.9698e-08 - recall: 1.0000\n",
      "Epoch 171/275\n",
      "75/75 - 0s - loss: 1.8748e-08 - recall: 1.0000\n",
      "Epoch 172/275\n",
      "75/75 - 0s - loss: 1.7786e-08 - recall: 1.0000\n",
      "Epoch 173/275\n",
      "75/75 - 0s - loss: 1.7062e-08 - recall: 1.0000\n",
      "Epoch 174/275\n",
      "75/75 - 0s - loss: 1.6125e-08 - recall: 1.0000\n",
      "Epoch 175/275\n",
      "75/75 - 0s - loss: 1.5226e-08 - recall: 1.0000\n",
      "Epoch 176/275\n",
      "75/75 - 0s - loss: 1.4464e-08 - recall: 1.0000\n",
      "Epoch 177/275\n",
      "75/75 - 0s - loss: 1.3740e-08 - recall: 1.0000\n",
      "Epoch 178/275\n",
      "75/75 - 0s - loss: 1.3053e-08 - recall: 1.0000\n",
      "Epoch 179/275\n",
      "75/75 - 0s - loss: 1.2316e-08 - recall: 1.0000\n",
      "Epoch 180/275\n",
      "75/75 - 0s - loss: 1.1654e-08 - recall: 1.0000\n",
      "Epoch 181/275\n",
      "75/75 - 0s - loss: 1.1092e-08 - recall: 1.0000\n",
      "Epoch 182/275\n",
      "75/75 - 0s - loss: 1.0492e-08 - recall: 1.0000\n",
      "Epoch 183/275\n",
      "75/75 - 0s - loss: 1.0005e-08 - recall: 1.0000\n",
      "Epoch 184/275\n",
      "75/75 - 0s - loss: 9.5178e-09 - recall: 1.0000\n",
      "Epoch 185/275\n",
      "75/75 - 0s - loss: 9.0181e-09 - recall: 1.0000\n",
      "Epoch 186/275\n",
      "75/75 - 0s - loss: 8.5185e-09 - recall: 1.0000\n",
      "Epoch 187/275\n",
      "75/75 - 0s - loss: 8.1313e-09 - recall: 1.0000\n",
      "Epoch 188/275\n",
      "75/75 - 0s - loss: 7.7941e-09 - recall: 1.0000\n",
      "Epoch 189/275\n",
      "75/75 - 0s - loss: 7.4069e-09 - recall: 1.0000\n",
      "Epoch 190/275\n",
      "75/75 - 0s - loss: 6.8073e-09 - recall: 1.0000\n",
      "Epoch 191/275\n",
      "75/75 - 0s - loss: 6.5200e-09 - recall: 1.0000\n",
      "Epoch 192/275\n",
      "75/75 - 0s - loss: 6.0829e-09 - recall: 1.0000\n",
      "Epoch 193/275\n",
      "75/75 - 0s - loss: 5.7706e-09 - recall: 1.0000\n",
      "Epoch 194/275\n",
      "75/75 - 0s - loss: 5.4833e-09 - recall: 1.0000\n",
      "Epoch 195/275\n",
      "75/75 - 0s - loss: 5.1711e-09 - recall: 1.0000\n",
      "Epoch 196/275\n",
      "75/75 - 0s - loss: 4.8963e-09 - recall: 1.0000\n",
      "Epoch 197/275\n",
      "75/75 - 0s - loss: 4.6590e-09 - recall: 1.0000\n",
      "Epoch 198/275\n",
      "75/75 - 0s - loss: 4.3217e-09 - recall: 1.0000\n",
      "Epoch 199/275\n",
      "75/75 - 0s - loss: 4.0844e-09 - recall: 1.0000\n",
      "Epoch 200/275\n",
      "75/75 - 0s - loss: 3.9220e-09 - recall: 1.0000\n",
      "Epoch 201/275\n",
      "75/75 - 0s - loss: 3.7222e-09 - recall: 1.0000\n",
      "Epoch 202/275\n",
      "75/75 - 0s - loss: 3.5473e-09 - recall: 1.0000\n",
      "Epoch 203/275\n",
      "75/75 - 0s - loss: 3.3225e-09 - recall: 1.0000\n",
      "Epoch 204/275\n",
      "75/75 - 0s - loss: 3.1726e-09 - recall: 1.0000\n",
      "Epoch 205/275\n",
      "75/75 - 0s - loss: 2.9478e-09 - recall: 1.0000\n",
      "Epoch 206/275\n",
      "75/75 - 0s - loss: 2.8353e-09 - recall: 1.0000\n",
      "Epoch 207/275\n",
      "75/75 - 0s - loss: 2.5730e-09 - recall: 1.0000\n",
      "Epoch 208/275\n",
      "75/75 - 0s - loss: 2.4731e-09 - recall: 1.0000\n",
      "Epoch 209/275\n",
      "75/75 - 0s - loss: 2.2858e-09 - recall: 1.0000\n",
      "Epoch 210/275\n",
      "75/75 - 0s - loss: 2.1359e-09 - recall: 1.0000\n",
      "Epoch 211/275\n",
      "75/75 - 0s - loss: 2.0235e-09 - recall: 1.0000\n",
      "Epoch 212/275\n",
      "75/75 - 0s - loss: 1.9360e-09 - recall: 1.0000\n",
      "Epoch 213/275\n",
      "75/75 - 0s - loss: 1.8486e-09 - recall: 1.0000\n",
      "Epoch 214/275\n",
      "75/75 - 0s - loss: 1.7362e-09 - recall: 1.0000\n",
      "Epoch 215/275\n",
      "75/75 - 0s - loss: 1.6238e-09 - recall: 1.0000\n",
      "Epoch 216/275\n",
      "75/75 - 0s - loss: 1.5363e-09 - recall: 1.0000\n",
      "Epoch 217/275\n",
      "75/75 - 0s - loss: 1.4239e-09 - recall: 1.0000\n",
      "Epoch 218/275\n",
      "75/75 - 0s - loss: 1.3864e-09 - recall: 1.0000\n",
      "Epoch 219/275\n",
      "75/75 - 0s - loss: 1.3115e-09 - recall: 1.0000\n",
      "Epoch 220/275\n",
      "75/75 - 0s - loss: 1.2490e-09 - recall: 1.0000\n",
      "Epoch 221/275\n",
      "75/75 - 0s - loss: 1.2116e-09 - recall: 1.0000\n",
      "Epoch 222/275\n",
      "75/75 - 0s - loss: 1.1741e-09 - recall: 1.0000\n",
      "Epoch 223/275\n",
      "75/75 - 0s - loss: 1.0742e-09 - recall: 1.0000\n",
      "Epoch 224/275\n",
      "75/75 - 0s - loss: 9.9924e-10 - recall: 1.0000\n",
      "Epoch 225/275\n",
      "75/75 - 0s - loss: 9.2430e-10 - recall: 1.0000\n",
      "Epoch 226/275\n",
      "75/75 - 0s - loss: 8.8683e-10 - recall: 1.0000\n",
      "Epoch 227/275\n",
      "75/75 - 0s - loss: 8.3686e-10 - recall: 1.0000\n",
      "Epoch 228/275\n",
      "75/75 - 0s - loss: 7.7441e-10 - recall: 1.0000\n",
      "Epoch 229/275\n",
      "75/75 - 0s - loss: 7.2445e-10 - recall: 1.0000\n",
      "Epoch 230/275\n",
      "75/75 - 0s - loss: 6.9947e-10 - recall: 1.0000\n",
      "Epoch 231/275\n",
      "75/75 - 0s - loss: 6.3702e-10 - recall: 1.0000\n",
      "Epoch 232/275\n",
      "75/75 - 0s - loss: 6.2452e-10 - recall: 1.0000\n",
      "Epoch 233/275\n",
      "75/75 - 0s - loss: 5.7456e-10 - recall: 1.0000\n",
      "Epoch 234/275\n",
      "75/75 - 0s - loss: 5.6207e-10 - recall: 1.0000\n",
      "Epoch 235/275\n",
      "75/75 - 0s - loss: 5.4958e-10 - recall: 1.0000\n",
      "Epoch 236/275\n",
      "75/75 - 0s - loss: 4.8713e-10 - recall: 1.0000\n",
      "Epoch 237/275\n",
      "75/75 - 0s - loss: 4.8713e-10 - recall: 1.0000\n",
      "Epoch 238/275\n",
      "75/75 - 0s - loss: 4.3717e-10 - recall: 1.0000\n",
      "Epoch 239/275\n",
      "75/75 - 0s - loss: 4.2468e-10 - recall: 1.0000\n",
      "Epoch 240/275\n",
      "75/75 - 0s - loss: 4.1219e-10 - recall: 1.0000\n",
      "Epoch 241/275\n",
      "75/75 - 0s - loss: 3.6222e-10 - recall: 1.0000\n",
      "Epoch 242/275\n",
      "75/75 - 0s - loss: 3.6222e-10 - recall: 1.0000\n",
      "Epoch 243/275\n",
      "75/75 - 0s - loss: 3.4973e-10 - recall: 1.0000\n",
      "Epoch 244/275\n",
      "75/75 - 0s - loss: 3.4973e-10 - recall: 1.0000\n",
      "Epoch 245/275\n",
      "75/75 - 0s - loss: 2.9977e-10 - recall: 1.0000\n",
      "Epoch 246/275\n",
      "75/75 - 0s - loss: 2.6230e-10 - recall: 1.0000\n",
      "Epoch 247/275\n",
      "75/75 - 0s - loss: 2.4981e-10 - recall: 1.0000\n",
      "Epoch 248/275\n",
      "75/75 - 0s - loss: 2.4981e-10 - recall: 1.0000\n",
      "Epoch 249/275\n",
      "75/75 - 0s - loss: 2.2483e-10 - recall: 1.0000\n",
      "Epoch 250/275\n",
      "75/75 - 0s - loss: 1.8736e-10 - recall: 1.0000\n",
      "Epoch 251/275\n",
      "75/75 - 0s - loss: 1.7487e-10 - recall: 1.0000\n",
      "Epoch 252/275\n",
      "75/75 - 0s - loss: 1.7487e-10 - recall: 1.0000\n",
      "Epoch 253/275\n",
      "75/75 - 0s - loss: 1.6238e-10 - recall: 1.0000\n",
      "Epoch 254/275\n",
      "75/75 - 0s - loss: 1.4989e-10 - recall: 1.0000\n",
      "Epoch 255/275\n",
      "75/75 - 0s - loss: 1.3740e-10 - recall: 1.0000\n",
      "Epoch 256/275\n",
      "75/75 - 0s - loss: 1.3740e-10 - recall: 1.0000\n",
      "Epoch 257/275\n",
      "75/75 - 0s - loss: 1.3740e-10 - recall: 1.0000\n",
      "Epoch 258/275\n",
      "75/75 - 0s - loss: 1.2490e-10 - recall: 1.0000\n",
      "Epoch 259/275\n",
      "75/75 - 0s - loss: 9.9924e-11 - recall: 1.0000\n",
      "Epoch 260/275\n",
      "75/75 - 0s - loss: 9.9924e-11 - recall: 1.0000\n",
      "Epoch 261/275\n",
      "75/75 - 0s - loss: 9.9924e-11 - recall: 1.0000\n",
      "Epoch 262/275\n",
      "75/75 - 0s - loss: 8.7433e-11 - recall: 1.0000\n",
      "Epoch 263/275\n",
      "75/75 - 0s - loss: 8.7433e-11 - recall: 1.0000\n",
      "Epoch 264/275\n",
      "75/75 - 0s - loss: 6.2452e-11 - recall: 1.0000\n",
      "Epoch 265/275\n",
      "75/75 - 0s - loss: 6.2452e-11 - recall: 1.0000\n",
      "Epoch 266/275\n",
      "75/75 - 0s - loss: 4.9962e-11 - recall: 1.0000\n",
      "Epoch 267/275\n",
      "75/75 - 0s - loss: 2.4981e-11 - recall: 1.0000\n",
      "Epoch 268/275\n",
      "75/75 - 0s - loss: 2.4981e-11 - recall: 1.0000\n",
      "Epoch 269/275\n",
      "75/75 - 0s - loss: 1.2490e-11 - recall: 1.0000\n",
      "Epoch 270/275\n",
      "75/75 - 0s - loss: 1.2490e-11 - recall: 1.0000\n",
      "Epoch 271/275\n",
      "75/75 - 0s - loss: 1.2490e-11 - recall: 1.0000\n",
      "Epoch 272/275\n",
      "75/75 - 0s - loss: 1.2490e-11 - recall: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/275\n",
      "75/75 - 0s - loss: 1.2490e-11 - recall: 1.0000\n",
      "Epoch 274/275\n",
      "75/75 - 0s - loss: 1.2490e-11 - recall: 1.0000\n",
      "Epoch 275/275\n",
      "75/75 - 0s - loss: 0.0000e+00 - recall: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fec1e2b4f28>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=275,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    workers=10, \n",
    "    batch_size =32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 - 0s - loss: 2.4889 - recall: 0.8073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.48885178565979, 0.8072625994682312]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 - 0s - loss: 2.4889 - recall: 0.8073\n",
      "Deep Neural Network - Loss: 2.48885178565979,  Recall: 0.8072625994682312 \n"
     ]
    }
   ],
   "source": [
    "model_loss, model_recall = deep_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Deep Neural Network - Loss: {model_loss},  Recall: {model_recall} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = deep_model.predict(X_test)\n",
    "#predictions_categorical = predictions.astype(int)\n",
    "#predictions = np.argmax(predictions_categorical, axis=-1)\n",
    "\n",
    "predictions = deep_model.predict(X_test) \n",
    "predictions = (predictions>0.25) # It will evaluate the logical expression y_predict>0.25 and return True or False\n",
    "predictions_categorical = predictions.astype(int)\n",
    "predictions = np.argmax(predictions_categorical, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_test\n",
    "labels = labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Diabetes       0.54      0.89      0.67       716\n",
      "Has Diabetes       0.69      0.26      0.37       716\n",
      "\n",
      "    accuracy                           0.57      1432\n",
      "   macro avg       0.62      0.57      0.52      1432\n",
      "weighted avg       0.62      0.57      0.52      1432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = labels\n",
    "y_pred = predictions\n",
    "target_names = [\"No Diabetes\", \"Has Diabetes\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_df3 = diab_df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert DIAABETES VALUES 0->1 AND 1->0\n",
    "\n",
    "diab_df3['diabetes'] = diab_df3['diabetes'].replace([0,1],[1,0])\n",
    "diab_df3['diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = diab_df3.drop(\"diabetes\", axis=1) \n",
    "feature_names3 = data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target3 = diab_df3[\"diabetes\"].values.reshape(-1, 1)\n",
    "target_names3 = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data3.shape)\n",
    "print(target3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data3, target3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler().fit(X_train)\n",
    "y_scaler = StandardScaler().fit(y_train)\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50, 75, 100],\n",
    "             # 'kernel':[\"linear\", \"rbf\",\"poly\"],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3, scoring = 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = grid.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"No Diabetes\", \"Has Diabetes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
